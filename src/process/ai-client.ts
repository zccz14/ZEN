import { AIMetadata } from '../types';
import { extractMetadataFromMarkdown } from '../ai/extractMetadataFromMarkdown';
import type { AIConfig } from './ai-utils';
import {
  getCachedMetadata,
  cacheMetadata,
  logTokenUsage,
} from './ai-utils';

/**
 * è°ƒç”¨ AI æ¨¡å‹æå–æ–‡æ¡£ metadata
 * @param content æ–‡æ¡£å†…å®¹
 * @param filePath æ–‡ä»¶è·¯å¾„
 * @param config AI é…ç½®ï¼ˆå¯é€‰ï¼‰
 * @returns æå–çš„ metadataï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› null
 */
export async function callAIForMetadata(
  content: string,
  filePath: string,
  config?: AIConfig
): Promise<AIMetadata | null> {
  // API key æ£€æŸ¥ç°åœ¨åœ¨ services/openai.ts ä¸­å¤„ç†
  // å¦‚æœ API key ä¸å­˜åœ¨ï¼ŒcompleteMessages å‡½æ•°ä¼šæŠ›å‡ºé”™è¯¯

  try {
    const metadata = await extractMetadataFromMarkdown(content, filePath);

    // æ‰“å° tokens ä½¿ç”¨æƒ…å†µ
    if (metadata.tokens_used) {
      logTokenUsage(filePath, metadata.tokens_used);
    }

    return metadata;
  } catch (error) {
    console.error(`âŒ Failed to extract AI metadata for ${filePath}:`, error);
    return null;
  }
}

/**
 * æ‰¹é‡è°ƒç”¨ AI å¤„ç†æ–‡ä»¶
 * @param files æ–‡ä»¶æ•°ç»„ï¼ŒåŒ…å«å†…å®¹ã€è·¯å¾„å’Œå“ˆå¸Œå€¼
 * @param config AI é…ç½®ï¼ˆå¯é€‰ï¼‰
 * @returns æ–‡ä»¶è·¯å¾„åˆ° metadata çš„æ˜ å°„
 */
export async function batchCallAI(
  files: Array<{ content: string; path: string; hash: string }>,
  config?: AIConfig
): Promise<Map<string, AIMetadata>> {
  const results = new Map<string, AIMetadata>();

  // API key æ£€æŸ¥ç°åœ¨åœ¨ services/openai.ts ä¸­å¤„ç†
  // å¦‚æœ API key ä¸å­˜åœ¨ï¼ŒcompleteMessages å‡½æ•°ä¼šæŠ›å‡ºé”™è¯¯

  console.log(`ğŸ¤– Processing ${files.length} files with AI...`);

  for (const file of files) {
    try {
      // æ£€æŸ¥ç¼“å­˜
      const cachedMetadata = await getCachedMetadata(file.hash, file.path);
      if (cachedMetadata) {
        results.set(file.path, cachedMetadata);
        continue;
      }

      // è°ƒç”¨ AI æå– metadata
      const metadata = await callAIForMetadata(file.content, file.path, config);
      if (metadata) {
        results.set(file.path, metadata);

        // ç¼“å­˜ç»“æœ
        await cacheMetadata(file.hash, file.path, metadata);
      }
    } catch (error) {
      console.error(`âŒ Failed to process file ${file.path}:`, error);
    }
  }

  return results;
}

/**
 * å¹¶è¡Œæ‰¹é‡è°ƒç”¨ AI å¤„ç†æ–‡ä»¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
 * @param files æ–‡ä»¶æ•°ç»„ï¼ŒåŒ…å«å†…å®¹ã€è·¯å¾„å’Œå“ˆå¸Œå€¼
 * @param config AI é…ç½®ï¼ˆå¯é€‰ï¼‰
 * @param concurrency å¹¶å‘æ•°ï¼ˆé»˜è®¤ 3ï¼‰
 * @returns æ–‡ä»¶è·¯å¾„åˆ° metadata çš„æ˜ å°„
 */
export async function batchCallAIParallel(
  files: Array<{ content: string; path: string; hash: string }>,
  config?: AIConfig,
  concurrency: number = 3
): Promise<Map<string, AIMetadata>> {
  const results = new Map<string, AIMetadata>();
  const queue = [...files];

  console.log(`ğŸ¤– Processing ${files.length} files with AI (parallel, concurrency: ${concurrency})...`);

  async function processBatch(batch: Array<{ content: string; path: string; hash: string }>) {
    const batchResults = new Map<string, AIMetadata>();
    const batchPromises = batch.map(async (file) => {
      try {
        // æ£€æŸ¥ç¼“å­˜
        const cachedMetadata = await getCachedMetadata(file.hash, file.path);
        if (cachedMetadata) {
          batchResults.set(file.path, cachedMetadata);
          return;
        }

        // è°ƒç”¨ AI æå– metadata
        const metadata = await callAIForMetadata(file.content, file.path, config);
        if (metadata) {
          batchResults.set(file.path, metadata);

          // ç¼“å­˜ç»“æœ
          await cacheMetadata(file.hash, file.path, metadata);
        }
      } catch (error) {
        console.error(`âŒ Failed to process file ${file.path}:`, error);
      }
    });

    await Promise.all(batchPromises);
    return batchResults;
  }

  while (queue.length > 0) {
    const batch = queue.splice(0, concurrency);
    const batchResults = await processBatch(batch);
    batchResults.forEach((metadata, path) => results.set(path, metadata));
  }

  return results;
}

/**
 * æ™ºèƒ½æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆæ ¹æ®æ–‡ä»¶å¤§å°å’Œæ•°é‡è‡ªåŠ¨é€‰æ‹©ç­–ç•¥ï¼‰
 * @param files æ–‡ä»¶æ•°ç»„ï¼ŒåŒ…å«å†…å®¹ã€è·¯å¾„å’Œå“ˆå¸Œå€¼
 * @param config AI é…ç½®ï¼ˆå¯é€‰ï¼‰
 * @returns æ–‡ä»¶è·¯å¾„åˆ° metadata çš„æ˜ å°„
 */
export async function smartBatchCallAI(
  files: Array<{ content: string; path: string; hash: string }>,
  config?: AIConfig
): Promise<Map<string, AIMetadata>> {
  // æ ¹æ®æ–‡ä»¶æ•°é‡å’Œæ€»å¤§å°é€‰æ‹©å¤„ç†ç­–ç•¥
  const totalSize = files.reduce((sum, file) => sum + file.content.length, 0);
  const avgSize = totalSize / files.length;

  if (files.length <= 5 || avgSize > 10000) {
    // æ–‡ä»¶æ•°é‡å°‘æˆ–å¹³å‡æ–‡ä»¶å¤§ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†
    return batchCallAI(files, config);
  } else {
    // æ–‡ä»¶æ•°é‡å¤šä¸”å¹³å‡æ–‡ä»¶å°ï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†
    const concurrency = Math.min(5, Math.ceil(files.length / 3));
    return batchCallAIParallel(files, config, concurrency);
  }
}

/**
 * åˆ›å»º AI å®¢æˆ·ç«¯å‡½æ•°é›†åˆï¼ˆé«˜é˜¶å‡½æ•°ï¼‰
 * @param config AI é…ç½®ï¼ˆå¯é€‰ï¼‰
 * @returns AI å®¢æˆ·ç«¯å‡½æ•°é›†åˆ
 */
export function createAIClient(config?: AIConfig) {
  return {
    extractMetadata: (content: string, filePath: string) =>
      callAIForMetadata(content, filePath, config),
    processFiles: (files: Array<{ content: string; path: string; hash: string }>) =>
      batchCallAI(files, config),
    processFilesParallel: (
      files: Array<{ content: string; path: string; hash: string }>,
      concurrency: number = 3
    ) => batchCallAIParallel(files, config, concurrency),
    processFilesSmart: (files: Array<{ content: string; path: string; hash: string }>) =>
      smartBatchCallAI(files, config),
  };
}

/**
 * åˆ›å»ºå¸¦ç¼“å­˜çš„ AI å®¢æˆ·ç«¯ï¼ˆé«˜é˜¶å‡½æ•°ï¼‰
 * @param cacheFunctions ç¼“å­˜å‡½æ•°é›†åˆ
 * @param config AI é…ç½®ï¼ˆå¯é€‰ï¼‰
 * @returns å¸¦ç¼“å­˜çš„ AI å®¢æˆ·ç«¯å‡½æ•°
 */
export function createCachedAIClient(
  cacheFunctions: {
    getCachedMetadata: (hash: string, path: string) => Promise<AIMetadata | null>;
    cacheMetadata: (hash: string, path: string, metadata: AIMetadata) => Promise<void>;
  },
  config?: AIConfig
) {
  return {
    extractMetadata: async (content: string, filePath: string, fileHash?: string) => {
      // å¦‚æœæœ‰æ–‡ä»¶å“ˆå¸Œï¼Œå…ˆæ£€æŸ¥ç¼“å­˜
      if (fileHash) {
        const cached = await cacheFunctions.getCachedMetadata(fileHash, filePath);
        if (cached) {
          return cached;
        }
      }

      // è°ƒç”¨ AI æå– metadata
      const metadata = await callAIForMetadata(content, filePath, config);

      // å¦‚æœæœ‰æ–‡ä»¶å“ˆå¸Œï¼Œç¼“å­˜ç»“æœ
      if (metadata && fileHash) {
        await cacheFunctions.cacheMetadata(fileHash, filePath, metadata);
      }

      return metadata;
    },
    processFiles: async (files: Array<{ content: string; path: string; hash: string }>) => {
      const results = new Map<string, AIMetadata>();

      for (const file of files) {
        const metadata = await cacheFunctions.getCachedMetadata(file.hash, file.path);
        if (metadata) {
          results.set(file.path, metadata);
        } else {
          const newMetadata = await callAIForMetadata(file.content, file.path, config);
          if (newMetadata) {
            results.set(file.path, newMetadata);
            await cacheFunctions.cacheMetadata(file.hash, file.path, newMetadata);
          }
        }
      }

      return results;
    },
  };
}